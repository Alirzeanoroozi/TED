# Model Configuration
model_name: "t5-3b"

# Data Paths
train_path: "dlp/extended_json_data/train.jsonl"
validation_path: "dlp/extended_json_data/validation.jsonl"

# CUDA Configuration
cuda_visible_devices: "0,1"

# BitsAndBytes 4-bit Quantization Configuration
load_in_4bit: true
bnb_4bit_use_double_quant: true
bnb_4bit_quant_type: "nf4"
bnb_4bit_compute_dtype: "bfloat16"

# LoRA Configuration
r: 16
lora_alpha: 32
target_modules: ["q", "v", "k", "o"]
lora_dropout: 0.1
bias: "none"

# Training Arguments
output_dir: "qlora_16_checkpoints/"
run_name: "QLoRA_1_Training"

# Batch Size Configuration
per_device_train_batch_size: 8
per_device_eval_batch_size: 32
gradient_accumulation_steps: 4

# Evaluation and Logging
eval_steps: 100
save_steps: 100
logging_steps: 10

# Learning Rate and Optimization
learning_rate: 1e-4
weight_decay: 0.01
max_grad_norm: 1.0

# Training Duration
num_train_epochs: 1
save_total_limit: 2

# DataLoader Optimization
dataloader_pin_memory: true
dataloader_num_workers: 4
