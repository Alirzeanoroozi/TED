# Data Paths
train_path: "dlp/jsons/ted_train.json"
validation_path: "dlp/jsons/ted_validation.json"

# CUDA Configuration
cuda_visible_devices: "0,1"

# Source and Target Maximum Sequence Length
src_max_seq_len: 2048
tgt_max_seq_len: 128

# Model Configuration
num_encoder_layers: 6
num_decoder_layers: 6
emb_size: 512
nhead: 8
dim_feedforward: 2048
dropout: 0.1
pad_token_id: 0
max_seq_len: 4096

# Training Arguments
output_dir: "transformer_checkpoints/"
run_name: "Transformer_Training"

# Batch Size Configuration
per_device_train_batch_size: 8
per_device_eval_batch_size: 32
gradient_accumulation_steps: 4

# Evaluation and Logging
eval_steps: 100
save_steps: 100
logging_steps: 10

# Learning Rate and Optimization
learning_rate: 1e-4
weight_decay: 0.01
max_grad_norm: 1.0

# Training Duration
num_train_epochs: 1
save_total_limit: 1
