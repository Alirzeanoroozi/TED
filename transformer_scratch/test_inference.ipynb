{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f590000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 8\n",
      "val_epochs 20\n",
      "lr 0.0001\n",
      "src_vocab_size 25\n",
      "tgt_vocab_size 20\n",
      "seq_len 1024\n",
      "d_model 960\n",
      "model_folder weights\n",
      "model_basename tmodel_\n",
      "preload latest\n",
      "experiment_name runs/tmodel\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from config import get_config\n",
    "config = get_config()\n",
    "# print(config)\n",
    "for k,v in config.items():\n",
    "    print(k,v)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8b691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_ds\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef1a7731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "-\n",
      "1\n",
      "2\n",
      "2\n",
      "_\n",
      "1\n",
      "6\n",
      "7\n",
      "-\n",
      "5\n",
      "8\n",
      "9\n",
      " \n",
      "|\n",
      " \n",
      "1\n",
      ".\n",
      "1\n",
      "0\n",
      ".\n",
      "6\n",
      "4\n",
      "0\n",
      ".\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer_tgt.encode(\"54-122_167-589 | 1.10.640.10\")\n",
    "# Encoding(num_tokens=28, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
    "encoded.ids\n",
    "for i in encoded.ids:\n",
    "    print(tokenizer_tgt.decode([i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eb51868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['encoder_input', 'decoder_input', 'encoder_mask', 'decoder_mask', 'label', 'src_text', 'tgt_text'])\n",
      "encoder_input.shape torch.Size([8, 1024])\n",
      "decoder_input.shape torch.Size([8, 1024])\n",
      "encoder_mask.shape torch.Size([8, 1, 1, 1024])\n",
      "decoder_mask.shape torch.Size([8, 1, 1024, 1024])\n",
      "label.shape torch.Size([8, 1024])\n",
      "['MGRYSREPDNPAKSCKARGSNLRVHFKNTYETAMAIKKLPLRRAVRYLKNVVDKKECIPFRRFNGGVGRCAQAKQFGTTQGRWPKKSAEF', 'ELEPVEPYSNEDLDWTDESSRVSREYENEDERDVELVSTEVDDWESYDTVFIGYPIWWAIAAWPVNDFVEANDFTGKTVIPFCTSATSGMGESGELLAELAGTGDWQEGQRFRSGADDKEIQEWVESLGL', 'MEGNTEKPFGIVRGITFSTMNMKRTNLAETEGNPGDQISTLIFRNVEWMGYSETLKISIAMLHLKR', 'MIAGTGIDIASISRMEKGLSRFGARFSRRLLSPGEETLFPAGTKRAAAFLAGRWAAKEAAVKALGTGFAGGVAPSDVSVLSLPSGQPFIRFAGRARERAEELGVTSVHVSITHDAGFAAAIVLLETGLR', 'MRAVTWQGRRDVRVETVPDPRIEEPTDVIVRVTSTGICGSDLHLYEPLGPFLDPGDILGHEPMGIVEEVGGAVTALKPGDRVVVPFNVSCGDCFMCDQGLQSQCETTQVTEYGTGAALFGYTKLYGQVPGGQAEYLRVPFGNTLPVKVGHGPPDDRYVYLSDVLPTAWQAVEYASVPRDGTVVVLGLGPIGDMAARIALHRGAGRVLGVDLVPDRLNRAAAHGVIPLDWRRYGKDLPEAVAEYTGGRGADAVIDAVGMEAHGSPVAKGAQRAVGLLPDAVAQPLMEHAGVDRLAALHMAMRLVRRGGTVSVSGVYGGALDPMPLLTMFDRQIQLRMGQANVLRWVPEILPLLDDEDVLGVDHFATHAMPLEEAPKAYAMFQEKADGMVKTLLKP', 'MIYCTSANPKASVMRKFWEGRCSLQCLEEWWHMPKLVGSLEQNFRDPRLGSAAVLLLLKSCILFVGSLRRGEPYVHVKPSNSLQLGIHRAVPRSSQNSPAKLLFSLSKLKKGGRVRDQYEARVVAVSGIILPGIAARGAAQ', 'MSEADPTPEQEGLEELLLGERPSLTRVEVAEAAGVPVEVAQELWRLLGFPATSDDDVAFTTADVEALRLAADLTGLGILGDDSRSALVRTWGRSFARLAEWQTTLMARVAAADPDADPAAQLAMLTGEVIPRVERLQSYVWRRHLTSAAGRRLAGSEGPSTNETTRQAVAFVDIVGFTSQSRSMREAELVGWIETFESRSTEVVVDHGGRVIKNIGDEVLLVADTPAAAARIVHLLVTMGADEDDPFPAVRAGVAYGDVVTRLGDVLGATVNIAARLTSLARPGTVLVDDGMREELEDSPVWSLRRVPRASVKGYSSLRPWALRHRD', 'MDNSGKEAEAMALLAEAERKVKNSQSFFSGLFGGSSKIEEACEIYARAANMFKMAKNWSAAGSAFCQAAQLHLQLQSKHDAATCFVDAGNAFKKADPQEAINCLMRAIEIYTDMGRFTIAAKHHISIAEIYETELVDIEKAIAHYEQSADYYKGEESNSSANKCLLKVAGYAAQLEQYQKAIDIYEQVGTNAMDSPLLKYSAKDYFFKAALCHFCIDMLNAKLAVQKYEELFPAFSDSRECKLMKKLLEAHEEQNVDSYTEAVKEYDSISRLDQWLTTMLLRIKKTIQGDEEDLR']\n",
      "['27-90 | 3.90.470.10', '46-129 | 3.40.50.360', '12-24_32-49 | -', '3-127 | 3.90.470.20', '1-162_339-391 | 3.40.50.720 * 163-335 | 3.90.180.10', '10-135 | -', '9-113 | 1.20.5 * 119-152 | 3.30.70.1230 * 165-326 | 1.10.1660', '211-295 | -']\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "# print(batch)\n",
    "print(batch.keys())\n",
    "# print(type(batch))\n",
    "encoder_input = batch['encoder_input'].to(device) # (B, seq_len)\n",
    "print(\"encoder_input.shape\", encoder_input.shape)\n",
    "decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "print(\"decoder_input.shape\", decoder_input.shape)\n",
    "encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "print(\"encoder_mask.shape\", encoder_mask.shape)\n",
    "decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "print(\"decoder_mask.shape\", decoder_mask.shape)\n",
    "label = batch['label'].to(device) # (B, seq_len)\n",
    "print(\"label.shape\", label.shape)\n",
    "source = batch['src_text']\n",
    "print(batch['src_text'])\n",
    "print(batch['tgt_text'])\n",
    "# print(decoder_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "585bff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading model weights/tmodel_400.pt\n"
     ]
    }
   ],
   "source": [
    "from model import build_transformer_esmc\n",
    "from config import latest_weights_file_path, get_weights_file_path\n",
    "model = build_transformer_esmc(tokenizer_tgt.get_vocab_size(), config['seq_len'], d_model=config['d_model']).to(device)\n",
    "\n",
    "preload = config['preload']\n",
    "model_filename = latest_weights_file_path(config)\n",
    "model_filename = \"weights/tmodel_400.pt\"\n",
    "if model_filename:\n",
    "    print(f'Preloading model {model_filename}')\n",
    "    state = torch.load(model_filename)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "else:\n",
    "    print('No model to preload, starting from scratch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29a5a9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 67,922,708\n"
     ]
    }
   ],
   "source": [
    "# Print the number of trainable parameters in the model and all the numbers\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "\n",
    "# # Print all parameter counts by module\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"{name}: {param.numel():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54b3dd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 960])\n",
      "tensor([[[ 0.0062, -0.0049,  0.0013,  ...,  0.0020, -0.0018, -0.0099],\n",
      "         [-0.0284,  0.0368, -0.0025,  ...,  0.0178, -0.0490,  0.0502],\n",
      "         [ 0.0160,  0.0379,  0.0275,  ...,  0.0014, -0.0507,  0.0165],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "encoder_output = model.encode(batch['src_text'], encoder_mask) # (B, seq_len, d_model)\n",
    "print(encoder_output.shape)\n",
    "print(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf3bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 960])\n",
      "tensor([[[-1.0622, -0.9160, -1.9601,  ...,  0.6036, -1.5966, -0.4590],\n",
      "         [-0.8933, -2.6225, -0.6266,  ...,  0.6246, -2.2994, -0.6068],\n",
      "         [-0.2721, -2.5294, -0.6001,  ...,  2.1170, -0.8606,  0.4062],\n",
      "         ...,\n",
      "         [-0.7625, -2.8094, -1.2348,  ...,  0.9257, -0.8632, -0.4945],\n",
      "         [-1.3344, -3.0120, -1.7427,  ...,  1.5122, -1.2151, -0.8662],\n",
      "         [-1.0707, -2.5780, -1.5908,  ...,  0.8420, -1.5378, -0.5994]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from model import build_transformer\n",
    "trans_model = build_transformer(tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size(), config['seq_len'], config['seq_len'], d_model=config['d_model']).to(device)\n",
    "encoder_output = trans_model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "print(encoder_output.shape)\n",
    "print(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ac0463c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['encoder_input', 'decoder_input', 'encoder_mask', 'decoder_mask', 'label', 'src_text', 'tgt_text'])\n",
      "encoder_input.shape torch.Size([1, 1024])\n",
      "decoder_input.shape torch.Size([1, 1024])\n",
      "encoder_mask.shape torch.Size([1, 1, 1, 1024])\n",
      "decoder_mask.shape torch.Size([1, 1, 1024, 1024])\n",
      "label.shape torch.Size([1, 1024])\n",
      "['MLINVLLTLWFYTAHGQGKPCSYPVIKHGRLYYSYRGYFPARVHQQFVYSCDHHFVPPSQRSWDHLTCTAEGWSPEEPCLRQCIFNYLENGHTPHREEKYLQGETVRVRCYEGYSLQNDQNTMTCTESGWSPPPRCIRVKTCSKSNIRIENGFLSESTFTYPLNKQTEYKCKPGYVTADGKTSGLITCLKNGWSAQPVCIKSCDRPVFEKARVKSDGTWFRLNDRLDYECVDGYENRDGRTTGSIVCGQDGWSDKAACYERECSIPEMDPYLNAYPRKETYKVGDVLKFSCSQGRIMVGADSVQCYHFGWSPKLPTCKVKSCALPPELPNGKRKEIHKEEYAHNEVVEYACNPKFLMKGSHKIQCVDGEWTALPVCIEEERTCGNIPDLDHGDVKPSVPPYHHGDSVEFSCREAFTMIGPRFITCISGEWTQPPQSKIQLCPPPPQVPNACDMTTTVNYQDGEKISILCKENYLIQDAEEIVCKGGRWQSIPRCIEKIGCSQPPQLDHGTINSSSSAEERREIHEQRLYAHGTKLSYTCEEGFEISENNVIICHMGKWSSPPQCVGLPCGLPPYIQNGVISHKKDSYQYGEEVTYDCDEGFGTDGPASIRCLGGEWSRPQDCISTNCVNLPTFEDAVLTDREKDFYRSGEKVAFKCLSYYQLDGSNTIQCIKSKWIGRPACRDVSCGNPPQVENAIIHNQKSRYQSDERARYECIGNYDLFGEMEVVCLNGTWTEPPQCKDSQGKCGPPPPIDNGYITSLLKSVYPTGSIVEYRCQAYYELRGNRNVVCRNGEWSQLPKCLEACTVSEETMRKHHIQLRWKHDKKIYSKTEDTIEFMCRYGYRELTPKHTFRATCREGKVVYPRCG']\n",
      "['30-79 | 2.10.70.10 * 84-139 | 2.10.70.10 * 142-204 | 2.10.70.10 * 206-261 | 2.10.70.10 * 264-319 | 2.10.70.10 * 332-379 | 2.10.70.10 * 383-438 | 2.10.70.10 * 628-684 | 2.10.70.10 * 696-741 | 2.10.70.10 * 756-802 | 2.10.70.10 * 805-856 | 2.10.70.10']\n",
      "0 1\n",
      "1 -\n",
      "2 1\n",
      "3 1\n",
      "4 3\n",
      "5  \n",
      "6 |\n",
      "7  \n",
      "8 3\n",
      "9 .\n",
      "10 4\n",
      "11 0\n",
      "12 .\n",
      "13 1\n",
      "14 0\n",
      "15 .\n",
      "16 1\n",
      "17 0\n",
      "18 \n",
      "tensor([ 2,  9,  6,  9,  9, 11,  4, 19,  4, 11,  7, 12,  8,  7,  9,  8,  7,  9,\n",
      "         8,  3], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 - 1 1 3   |   3 . 4 0 . 1 0 . 1 0'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import causal_mask\n",
    "\n",
    "batch = next(iter(val_dataloader))\n",
    "# print(batch)\n",
    "print(batch.keys())\n",
    "# print(type(batch))\n",
    "encoder_input = batch['encoder_input'].to(device) # (B, seq_len)\n",
    "print(\"encoder_input.shape\", encoder_input.shape)\n",
    "decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "print(\"decoder_input.shape\", decoder_input.shape)\n",
    "encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "print(\"encoder_mask.shape\", encoder_mask.shape)\n",
    "decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "print(\"decoder_mask.shape\", decoder_mask.shape)\n",
    "label = batch['label'].to(device) # (B, seq_len)\n",
    "print(\"label.shape\", label.shape)\n",
    "source = batch['src_text']\n",
    "print(batch['src_text'])\n",
    "print(batch['tgt_text'])\n",
    "\n",
    "\n",
    "\n",
    "sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "# Precompute the encoder output and reuse it for every step\n",
    "encoder_output = model.encode(source, encoder_mask)\n",
    "# Initialize the decoder input with the sos token\n",
    "decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(encoder_input).to(device)\n",
    "i = 0\n",
    "while True:\n",
    "    if decoder_input.size(1) == 30:\n",
    "        break\n",
    "\n",
    "    # build mask for target\n",
    "    decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_input).to(device)\n",
    "\n",
    "    # calculate output\n",
    "    out = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "\n",
    "    # get next token\n",
    "    prob = model.project(out[:, -1])\n",
    "    _, next_word = torch.max(prob, dim=1)\n",
    "    tgt_token = tokenizer_tgt.decode([next_word.item()])\n",
    "    print(i,tgt_token)\n",
    "    decoder_input = torch.cat(\n",
    "        [decoder_input, torch.empty(1, 1).fill_(next_word.item()).type_as(encoder_input).to(device)], dim=1\n",
    "    )\n",
    "\n",
    "    if next_word == eos_idx:\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "print(decoder_input.squeeze(0))\n",
    "tokenizer_tgt.decode(decoder_input.squeeze(0).tolist())\n",
    "# return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916dd247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pooriya/anaconda3/envs/new_ted/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 7297.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading model weights/tmodel__latest.pt\n",
      "0 done\n",
      "1 done\n",
      "2 done\n",
      "3 done\n",
      "4 done\n",
      "5 done\n",
      "6 done\n",
      "7 done\n",
      "8 done\n",
      "9 done\n",
      "10 done\n",
      "11 done\n",
      "12 done\n",
      "13 done\n",
      "14 done\n",
      "15 done\n",
      "16 done\n",
      "17 done\n",
      "18 done\n",
      "19 done\n",
      "20 done\n",
      "21 done\n",
      "22 done\n",
      "23 done\n",
      "24 done\n",
      "25 done\n",
      "26 done\n",
      "27 done\n",
      "28 done\n",
      "29 done\n",
      "30 done\n",
      "31 done\n",
      "32 done\n",
      "33 done\n",
      "34 done\n",
      "35 done\n",
      "36 done\n",
      "37 done\n",
      "38 done\n",
      "39 done\n",
      "40 done\n",
      "41 done\n",
      "42 done\n",
      "43 done\n",
      "44 done\n",
      "45 done\n",
      "46 done\n",
      "47 done\n",
      "48 done\n",
      "49 done\n",
      "50 done\n",
      "51 done\n",
      "52 done\n",
      "53 done\n",
      "54 done\n",
      "55 done\n",
      "56 done\n",
      "57 done\n",
      "58 done\n",
      "59 done\n",
      "60 done\n",
      "61 done\n",
      "62 done\n",
      "63 done\n",
      "64 done\n",
      "65 done\n",
      "66 done\n",
      "67 done\n",
      "68 done\n",
      "69 done\n",
      "70 done\n",
      "71 done\n",
      "72 done\n",
      "73 done\n",
      "74 done\n",
      "75 done\n",
      "76 done\n",
      "77 done\n",
      "78 done\n",
      "79 done\n",
      "80 done\n",
      "81 done\n",
      "82 done\n",
      "83 done\n",
      "84 done\n",
      "85 done\n",
      "86 done\n",
      "87 done\n",
      "88 done\n",
      "89 done\n",
      "90 done\n",
      "91 done\n",
      "92 done\n",
      "93 done\n",
      "94 done\n",
      "95 done\n",
      "96 done\n",
      "97 done\n",
      "98 done\n",
      "99 done\n",
      "100 done\n",
      "101 done\n",
      "102 done\n",
      "103 done\n",
      "104 done\n",
      "105 done\n",
      "106 done\n",
      "107 done\n",
      "108 done\n",
      "109 done\n",
      "0.26833263251743167\n"
     ]
    }
   ],
   "source": [
    "from model import build_transformer_esmc\n",
    "from dataset import causal_mask\n",
    "from config import get_config, get_weights_file_path\n",
    "from dataloader import get_ds\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Define the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "device = torch.device(device)\n",
    "\n",
    "def greedy_decode(model, source, source_mask, tokenizer_tgt, max_len):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type(torch.int64).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).fill_(next_word.item()).type(torch.int64).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "def run_validation(model, validation_ds, tokenizer_tgt, max_len):\n",
    "    model.eval()\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(validation_ds):\n",
    "            print(i, \"done\")\n",
    "            model_out = greedy_decode(model, batch[\"src_text\"], batch[\"encoder_mask\"].to(device), tokenizer_tgt, max_len)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = \"\"\n",
    "            for i in model_out.detach().tolist():\n",
    "                model_out_text += tokenizer_tgt.decode([i])\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "    \n",
    "    # Calculate character-wise accuracy for each prediction\n",
    "    def charwise_acc(pred, exp):\n",
    "        if not exp:\n",
    "            return 0.0\n",
    "        # Pad the shorter string so zip works for all chars\n",
    "        max_len = max(len(pred), len(exp))\n",
    "        pred = pred.ljust(max_len)\n",
    "        exp = exp.ljust(max_len)\n",
    "        correct = sum(p == e for p, e in zip(pred, exp))\n",
    "        return correct / max_len\n",
    "\n",
    "    charwise_accs = [charwise_acc(p, e) for p, e in zip(predicted, expected)]\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    table = pd.DataFrame({\n",
    "        \"input\": source_texts,\n",
    "        \"label\": expected,\n",
    "        \"predicted\": predicted,\n",
    "        \"charwise_accuracy\": charwise_accs\n",
    "    })\n",
    "\n",
    "    # Save the table to a CSV file\n",
    "    table.to_csv(\"../results/eval_samples.csv\", index=False)\n",
    "    return table\n",
    "\n",
    "\n",
    "config = get_config()\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "model = build_transformer_esmc(tokenizer_tgt.get_vocab_size(), config['tgt_seq_len'], d_model=config['d_model']).to(device)\n",
    "\n",
    "# If the user specified a model to preload before training, load it\n",
    "model_filename = get_weights_file_path(config) if config['preload'] else None\n",
    "if model_filename:\n",
    "    print(f'Preloading model {model_filename}')\n",
    "    state = torch.load(model_filename)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "else:\n",
    "    print('No model to preload, starting from scratch')\n",
    "\n",
    "df = run_validation(model, val_dataloader, tokenizer_tgt, 100)\n",
    "print(sum(df[\"charwise_accuracy\"]) / len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914a6b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17634420020401081\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../results/eval_samples.csv\")\n",
    "print(sum(df[\"charwise_accuracy\"]) / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ca48aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41973127191310583\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../results/results.csv\")\n",
    "print(sum(df[\"token_accuracy\"]) / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57193dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17876832055200503\n"
     ]
    }
   ],
   "source": [
    "# label,new_pred\n",
    "df = pd.read_csv(\"../results/results_new_2column.csv\")\n",
    "def charwise_acc(pred, exp):\n",
    "    if not exp:\n",
    "        return 0.0\n",
    "    # Pad the shorter string so zip works for all chars\n",
    "    max_len = max(len(pred), len(exp))\n",
    "    pred = pred.ljust(max_len)\n",
    "    exp = exp.ljust(max_len)\n",
    "    correct = sum(p == e for p, e in zip(pred, exp))\n",
    "    return correct / max_len\n",
    "\n",
    "charwise_accs = [charwise_acc(p, e) for p, e in zip(df[\"new_pred\"], df[\"label\"])]\n",
    "\n",
    "print(sum(charwise_accs) / len(charwise_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4070c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_ted",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
