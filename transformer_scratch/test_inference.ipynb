{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f590000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 8\n",
      "val_epochs 20\n",
      "lr 0.0001\n",
      "src_vocab_size 25\n",
      "tgt_vocab_size 20\n",
      "seq_len 1024\n",
      "d_model 960\n",
      "model_folder weights\n",
      "model_basename tmodel_\n",
      "preload latest\n",
      "experiment_name runs/tmodel\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from config import get_config\n",
    "config = get_config()\n",
    "# print(config)\n",
    "for k,v in config.items():\n",
    "    print(k,v)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8b691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_ds\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef1a7731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "-\n",
      "1\n",
      "2\n",
      "2\n",
      "_\n",
      "1\n",
      "6\n",
      "7\n",
      "-\n",
      "5\n",
      "8\n",
      "9\n",
      " \n",
      "|\n",
      " \n",
      "1\n",
      ".\n",
      "1\n",
      "0\n",
      ".\n",
      "6\n",
      "4\n",
      "0\n",
      ".\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer_tgt.encode(\"54-122_167-589 | 1.10.640.10\")\n",
    "# Encoding(num_tokens=28, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
    "encoded.ids\n",
    "for i in encoded.ids:\n",
    "    print(tokenizer_tgt.decode([i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eb51868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['encoder_input', 'decoder_input', 'encoder_mask', 'decoder_mask', 'label', 'src_text', 'tgt_text'])\n",
      "encoder_input.shape torch.Size([8, 1024])\n",
      "decoder_input.shape torch.Size([8, 1024])\n",
      "encoder_mask.shape torch.Size([8, 1, 1, 1024])\n",
      "decoder_mask.shape torch.Size([8, 1, 1024, 1024])\n",
      "label.shape torch.Size([8, 1024])\n",
      "['MGRYSREPDNPAKSCKARGSNLRVHFKNTYETAMAIKKLPLRRAVRYLKNVVDKKECIPFRRFNGGVGRCAQAKQFGTTQGRWPKKSAEF', 'ELEPVEPYSNEDLDWTDESSRVSREYENEDERDVELVSTEVDDWESYDTVFIGYPIWWAIAAWPVNDFVEANDFTGKTVIPFCTSATSGMGESGELLAELAGTGDWQEGQRFRSGADDKEIQEWVESLGL', 'MEGNTEKPFGIVRGITFSTMNMKRTNLAETEGNPGDQISTLIFRNVEWMGYSETLKISIAMLHLKR', 'MIAGTGIDIASISRMEKGLSRFGARFSRRLLSPGEETLFPAGTKRAAAFLAGRWAAKEAAVKALGTGFAGGVAPSDVSVLSLPSGQPFIRFAGRARERAEELGVTSVHVSITHDAGFAAAIVLLETGLR', 'MRAVTWQGRRDVRVETVPDPRIEEPTDVIVRVTSTGICGSDLHLYEPLGPFLDPGDILGHEPMGIVEEVGGAVTALKPGDRVVVPFNVSCGDCFMCDQGLQSQCETTQVTEYGTGAALFGYTKLYGQVPGGQAEYLRVPFGNTLPVKVGHGPPDDRYVYLSDVLPTAWQAVEYASVPRDGTVVVLGLGPIGDMAARIALHRGAGRVLGVDLVPDRLNRAAAHGVIPLDWRRYGKDLPEAVAEYTGGRGADAVIDAVGMEAHGSPVAKGAQRAVGLLPDAVAQPLMEHAGVDRLAALHMAMRLVRRGGTVSVSGVYGGALDPMPLLTMFDRQIQLRMGQANVLRWVPEILPLLDDEDVLGVDHFATHAMPLEEAPKAYAMFQEKADGMVKTLLKP', 'MIYCTSANPKASVMRKFWEGRCSLQCLEEWWHMPKLVGSLEQNFRDPRLGSAAVLLLLKSCILFVGSLRRGEPYVHVKPSNSLQLGIHRAVPRSSQNSPAKLLFSLSKLKKGGRVRDQYEARVVAVSGIILPGIAARGAAQ', 'MSEADPTPEQEGLEELLLGERPSLTRVEVAEAAGVPVEVAQELWRLLGFPATSDDDVAFTTADVEALRLAADLTGLGILGDDSRSALVRTWGRSFARLAEWQTTLMARVAAADPDADPAAQLAMLTGEVIPRVERLQSYVWRRHLTSAAGRRLAGSEGPSTNETTRQAVAFVDIVGFTSQSRSMREAELVGWIETFESRSTEVVVDHGGRVIKNIGDEVLLVADTPAAAARIVHLLVTMGADEDDPFPAVRAGVAYGDVVTRLGDVLGATVNIAARLTSLARPGTVLVDDGMREELEDSPVWSLRRVPRASVKGYSSLRPWALRHRD', 'MDNSGKEAEAMALLAEAERKVKNSQSFFSGLFGGSSKIEEACEIYARAANMFKMAKNWSAAGSAFCQAAQLHLQLQSKHDAATCFVDAGNAFKKADPQEAINCLMRAIEIYTDMGRFTIAAKHHISIAEIYETELVDIEKAIAHYEQSADYYKGEESNSSANKCLLKVAGYAAQLEQYQKAIDIYEQVGTNAMDSPLLKYSAKDYFFKAALCHFCIDMLNAKLAVQKYEELFPAFSDSRECKLMKKLLEAHEEQNVDSYTEAVKEYDSISRLDQWLTTMLLRIKKTIQGDEEDLR']\n",
      "['27-90 | 3.90.470.10', '46-129 | 3.40.50.360', '12-24_32-49 | -', '3-127 | 3.90.470.20', '1-162_339-391 | 3.40.50.720 * 163-335 | 3.90.180.10', '10-135 | -', '9-113 | 1.20.5 * 119-152 | 3.30.70.1230 * 165-326 | 1.10.1660', '211-295 | -']\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "# print(batch)\n",
    "print(batch.keys())\n",
    "# print(type(batch))\n",
    "encoder_input = batch['encoder_input'].to(device) # (B, seq_len)\n",
    "print(\"encoder_input.shape\", encoder_input.shape)\n",
    "decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "print(\"decoder_input.shape\", decoder_input.shape)\n",
    "encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "print(\"encoder_mask.shape\", encoder_mask.shape)\n",
    "decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "print(\"decoder_mask.shape\", decoder_mask.shape)\n",
    "label = batch['label'].to(device) # (B, seq_len)\n",
    "print(\"label.shape\", label.shape)\n",
    "source = batch['src_text']\n",
    "print(batch['src_text'])\n",
    "print(batch['tgt_text'])\n",
    "# print(decoder_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "585bff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading model weights/tmodel_400.pt\n"
     ]
    }
   ],
   "source": [
    "from model import build_transformer_esmc\n",
    "from config import latest_weights_file_path, get_weights_file_path\n",
    "model = build_transformer_esmc(tokenizer_tgt.get_vocab_size(), config['seq_len'], d_model=config['d_model']).to(device)\n",
    "\n",
    "preload = config['preload']\n",
    "model_filename = latest_weights_file_path(config)\n",
    "model_filename = \"weights/tmodel_400.pt\"\n",
    "if model_filename:\n",
    "    print(f'Preloading model {model_filename}')\n",
    "    state = torch.load(model_filename)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "else:\n",
    "    print('No model to preload, starting from scratch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29a5a9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 67,922,708\n"
     ]
    }
   ],
   "source": [
    "# Print the number of trainable parameters in the model and all the numbers\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "\n",
    "# # Print all parameter counts by module\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"{name}: {param.numel():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54b3dd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 960])\n",
      "tensor([[[ 0.0062, -0.0049,  0.0013,  ...,  0.0020, -0.0018, -0.0099],\n",
      "         [-0.0284,  0.0368, -0.0025,  ...,  0.0178, -0.0490,  0.0502],\n",
      "         [ 0.0160,  0.0379,  0.0275,  ...,  0.0014, -0.0507,  0.0165],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "encoder_output = model.encode(batch['src_text'], encoder_mask) # (B, seq_len, d_model)\n",
    "print(encoder_output.shape)\n",
    "print(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf3bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 960])\n",
      "tensor([[[-1.0622, -0.9160, -1.9601,  ...,  0.6036, -1.5966, -0.4590],\n",
      "         [-0.8933, -2.6225, -0.6266,  ...,  0.6246, -2.2994, -0.6068],\n",
      "         [-0.2721, -2.5294, -0.6001,  ...,  2.1170, -0.8606,  0.4062],\n",
      "         ...,\n",
      "         [-0.7625, -2.8094, -1.2348,  ...,  0.9257, -0.8632, -0.4945],\n",
      "         [-1.3344, -3.0120, -1.7427,  ...,  1.5122, -1.2151, -0.8662],\n",
      "         [-1.0707, -2.5780, -1.5908,  ...,  0.8420, -1.5378, -0.5994]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from model import build_transformer\n",
    "trans_model = build_transformer(tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size(), config['seq_len'], config['seq_len'], d_model=config['d_model']).to(device)\n",
    "encoder_output = trans_model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "print(encoder_output.shape)\n",
    "print(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ac0463c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['encoder_input', 'decoder_input', 'encoder_mask', 'decoder_mask', 'label', 'src_text', 'tgt_text'])\n",
      "encoder_input.shape torch.Size([1, 1024])\n",
      "decoder_input.shape torch.Size([1, 1024])\n",
      "encoder_mask.shape torch.Size([1, 1, 1, 1024])\n",
      "decoder_mask.shape torch.Size([1, 1, 1024, 1024])\n",
      "label.shape torch.Size([1, 1024])\n",
      "['MLINVLLTLWFYTAHGQGKPCSYPVIKHGRLYYSYRGYFPARVHQQFVYSCDHHFVPPSQRSWDHLTCTAEGWSPEEPCLRQCIFNYLENGHTPHREEKYLQGETVRVRCYEGYSLQNDQNTMTCTESGWSPPPRCIRVKTCSKSNIRIENGFLSESTFTYPLNKQTEYKCKPGYVTADGKTSGLITCLKNGWSAQPVCIKSCDRPVFEKARVKSDGTWFRLNDRLDYECVDGYENRDGRTTGSIVCGQDGWSDKAACYERECSIPEMDPYLNAYPRKETYKVGDVLKFSCSQGRIMVGADSVQCYHFGWSPKLPTCKVKSCALPPELPNGKRKEIHKEEYAHNEVVEYACNPKFLMKGSHKIQCVDGEWTALPVCIEEERTCGNIPDLDHGDVKPSVPPYHHGDSVEFSCREAFTMIGPRFITCISGEWTQPPQSKIQLCPPPPQVPNACDMTTTVNYQDGEKISILCKENYLIQDAEEIVCKGGRWQSIPRCIEKIGCSQPPQLDHGTINSSSSAEERREIHEQRLYAHGTKLSYTCEEGFEISENNVIICHMGKWSSPPQCVGLPCGLPPYIQNGVISHKKDSYQYGEEVTYDCDEGFGTDGPASIRCLGGEWSRPQDCISTNCVNLPTFEDAVLTDREKDFYRSGEKVAFKCLSYYQLDGSNTIQCIKSKWIGRPACRDVSCGNPPQVENAIIHNQKSRYQSDERARYECIGNYDLFGEMEVVCLNGTWTEPPQCKDSQGKCGPPPPIDNGYITSLLKSVYPTGSIVEYRCQAYYELRGNRNVVCRNGEWSQLPKCLEACTVSEETMRKHHIQLRWKHDKKIYSKTEDTIEFMCRYGYRELTPKHTFRATCREGKVVYPRCG']\n",
      "['30-79 | 2.10.70.10 * 84-139 | 2.10.70.10 * 142-204 | 2.10.70.10 * 206-261 | 2.10.70.10 * 264-319 | 2.10.70.10 * 332-379 | 2.10.70.10 * 383-438 | 2.10.70.10 * 628-684 | 2.10.70.10 * 696-741 | 2.10.70.10 * 756-802 | 2.10.70.10 * 805-856 | 2.10.70.10']\n",
      "0 1\n",
      "1 -\n",
      "2 1\n",
      "3 1\n",
      "4 3\n",
      "5  \n",
      "6 |\n",
      "7  \n",
      "8 3\n",
      "9 .\n",
      "10 4\n",
      "11 0\n",
      "12 .\n",
      "13 1\n",
      "14 0\n",
      "15 .\n",
      "16 1\n",
      "17 0\n",
      "18 \n",
      "tensor([ 2,  9,  6,  9,  9, 11,  4, 19,  4, 11,  7, 12,  8,  7,  9,  8,  7,  9,\n",
      "         8,  3], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 - 1 1 3   |   3 . 4 0 . 1 0 . 1 0'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import causal_mask\n",
    "\n",
    "batch = next(iter(val_dataloader))\n",
    "# print(batch)\n",
    "print(batch.keys())\n",
    "# print(type(batch))\n",
    "encoder_input = batch['encoder_input'].to(device) # (B, seq_len)\n",
    "print(\"encoder_input.shape\", encoder_input.shape)\n",
    "decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "print(\"decoder_input.shape\", decoder_input.shape)\n",
    "encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "print(\"encoder_mask.shape\", encoder_mask.shape)\n",
    "decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "print(\"decoder_mask.shape\", decoder_mask.shape)\n",
    "label = batch['label'].to(device) # (B, seq_len)\n",
    "print(\"label.shape\", label.shape)\n",
    "source = batch['src_text']\n",
    "print(batch['src_text'])\n",
    "print(batch['tgt_text'])\n",
    "\n",
    "\n",
    "\n",
    "sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "# Precompute the encoder output and reuse it for every step\n",
    "encoder_output = model.encode(source, encoder_mask)\n",
    "# Initialize the decoder input with the sos token\n",
    "decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(encoder_input).to(device)\n",
    "i = 0\n",
    "while True:\n",
    "    if decoder_input.size(1) == 30:\n",
    "        break\n",
    "\n",
    "    # build mask for target\n",
    "    decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_input).to(device)\n",
    "\n",
    "    # calculate output\n",
    "    out = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "\n",
    "    # get next token\n",
    "    prob = model.project(out[:, -1])\n",
    "    _, next_word = torch.max(prob, dim=1)\n",
    "    tgt_token = tokenizer_tgt.decode([next_word.item()])\n",
    "    print(i,tgt_token)\n",
    "    decoder_input = torch.cat(\n",
    "        [decoder_input, torch.empty(1, 1).fill_(next_word.item()).type_as(encoder_input).to(device)], dim=1\n",
    "    )\n",
    "\n",
    "    if next_word == eos_idx:\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "print(decoder_input.squeeze(0))\n",
    "tokenizer_tgt.decode(decoder_input.squeeze(0).tolist())\n",
    "# return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916dd247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_ted",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
